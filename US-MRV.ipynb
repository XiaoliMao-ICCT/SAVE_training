{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is designed specifically for the US-MRV project\n",
    "* identify ships that originated/departed U.S. in 2022\n",
    "* apply voyage identification to these ships\n",
    "* filter out voyages from and to U.S. ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please ensure that you have all packages properly installed \n",
    "import boto3\n",
    "from io import BytesIO,StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import movingpandas as mpd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import shapely\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPoint, shape\n",
    "from geopy.distance import great_circle\n",
    "import shapefile\n",
    "import rtree\n",
    "import sys\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from holoviews import opts, dim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all inputs needed for this project is stored in AWS S3 bucket named \"miscellaneous-2024\"\n",
    "# use the following credentials to connect to our AWS S3 bucket\n",
    "aws_id=\"AKIAJUFZOOTYNKNWHBLQ\"\n",
    "aws_secret=\"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\"\n",
    "s3=boto3.client('s3',aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
    "\n",
    "# Input: IHS shipdatabse.\n",
    "# the most recent ihs database in named ihs_2022_process.csv\n",
    "obj=s3.get_object(Bucket=\"miscellaneous-2024\",Key=\"ihs_2022_process.csv\")\n",
    "ihs=pd.read_csv(obj['Body'])\n",
    "\n",
    "# Input: ships that visited US in 2022.\n",
    "# The green steel projected used ships traded with the US in 2022, which is considered the same.\n",
    "obj=s3.get_object(Bucket=\"miscellaneous-2024\",Key=\"usmrv_ship_2022.csv\")\n",
    "ships_us_2022=pd.read_csv(obj['Body'])\n",
    "\n",
    "# Input: U.S.ports shapefile\n",
    "# The US port shore power project generates a more detailed US port shapefile which is used in this project\n",
    "port_us=gpd.read_file('s3://miscellaneous-2024/USPorts_5nm_Buffers/Buffers_merged.shp')\n",
    "port_all=gpd.read_file('s3://save-10-year/Shapefiles/worldports_buffer_5nm_wSpecialPorts.shp')\n",
    "port_eu=port_all.loc[port_all.EU_port==1]\n",
    "\n",
    "# Input: 2022 SAVE outputs\n",
    "# use the following credentials to connect to our PostgreSQL database\n",
    "try:\n",
    "    con = psycopg2.connect(host=\"ais-global-2017-2019.cdf3dcxncw9p.us-east-1.rds.amazonaws.com\", port=5432, database=\"ais_2022\",user=\"postgres\",password=\"MarineTeamR0cks!\")\n",
    "    print(\"I have successfully connected to the database\")\n",
    "except Exception as e:\n",
    "    print(\"I am unable to connect to the database:\" +str(e))\n",
    "# To retreive data for each ship from the database, we use a pre-defined function, at the bottom.\n",
    "# Please run that code before moving to the next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voyage identification code\n",
    "* The algorithm is defined as a function, at the bottom of this notebook. To use that function, run that cell first.\n",
    "* The code can be found in GitHub account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "df_final=[]\n",
    "usmrv_lid_outbound=[]\n",
    "usmrv_lid_inbound=[]\n",
    "usmrv_lid_cabotage=[]\n",
    "usmrv_lid_eumrv=[]\n",
    "for i in ships_us_2022.ship_id:\n",
    "    count+=1\n",
    "    df=get_ship_from_pg(i,2022) # retrieve annual hourly processed data for ship i\n",
    "    df=df.rename(columns={'distancetravelled':'distancetravelled_nm'})\n",
    "    voyage=get_traj_and_split(df) # apply voyage identification\n",
    "\n",
    "    # next we find origins and destinations of each voyage and intersect them with shapefiles of interest to filter the routes we want\n",
    "    voyage['time']=pd.to_datetime(voyage['time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    crs = 'epsg:4326'\n",
    "    geometry = [Point(xy) for xy in zip(voyage['lon'], voyage['lat'])]\n",
    "    geo_df= gpd.GeoDataFrame(voyage, crs = crs, geometry = geometry)\n",
    "    geo_df= geo_df.set_index('time')\n",
    "    trips_new=mpd.TrajectoryCollection(geo_df,traj_id_col='lid') # mpd, or movingpandas is a python package to generate trajectory data\n",
    "    if len(trips_new)>0:\n",
    "        origins=trips_new.get_start_locations()[['lid','geometry']]\n",
    "        destinations=trips_new.get_end_locations()[['lid','geometry']]\n",
    "        voyage=voyage.merge(origins,on='lid',how='left')\n",
    "        voyage=voyage.rename(columns={'geometry_y':'origin_geometry'})\n",
    "        voyage=voyage.merge(destinations,on='lid',how='left')\n",
    "        voyage=voyage.rename(columns={'geometry':'destination_geometry'})\n",
    "        crs = 'epsg:4326'\n",
    "    \n",
    "        # voyages that originates in US ports or eu ports\n",
    "        geo_df = gpd.GeoDataFrame(voyage, crs = crs, geometry = voyage['origin_geometry'])\n",
    "        port_us_gridded['o_in_usport'] = [1]*len(port_us_gridded)\n",
    "        port_eu['o_in_euport'] = [1]*len(port_eu)\n",
    "        \n",
    "        print('Now completing us port intersection: ' + str(datetime.now()))\n",
    "        try:\n",
    "                points_with_intersection = gpd.sjoin(geo_df, \n",
    "                                            port_us_gridded[['geometry','PORT_NAME','o_in_usport']].rename(columns = {'PORT_NAME':'o_port_name_us'}), \n",
    "                                            how='left', op='intersects').reset_index().drop_duplicates(subset='index').set_index('index')\n",
    "                del points_with_intersection['index_right']\n",
    "        except:\n",
    "                points_with_intersection = geo_df\n",
    "                points_with_intersection['o_in_usport'] = 0\n",
    "            \n",
    "        print('Now completing eu port intersection: ' + str(datetime.now()))\n",
    "        try:\n",
    "                points_with_intersection = gpd.sjoin(points_with_intersection, \n",
    "                                            port_eu[['geometry','PORT_NAME','o_in_euport']].rename(columns = {'PORT_NAME':'o_port_name_eu'}), \n",
    "                                            how='left', op='intersects').reset_index().drop_duplicates(subset='index').set_index('index')\n",
    "                del points_with_intersection['index_right']\n",
    "        except:\n",
    "                # points_with_intersection = points_with_intersection\n",
    "                points_with_intersection['o_in_euport'] = 0\n",
    "\n",
    "        voyage=points_with_intersection\n",
    "        voyage.loc[voyage.o_in_usport.isnull(),'o_in_usport']=0\n",
    "        voyage.loc[voyage.o_in_euport.isnull(),'o_in_euport']=0\n",
    "        # del voyage['origin_geometry']\n",
    "        \n",
    "        # voyages that destine in US ports or eu ports\n",
    "        geo_df = gpd.GeoDataFrame(voyage, crs = crs, geometry = voyage['destination_geometry'])\n",
    "        port_us_gridded['d_in_usport'] = [1]*len(port_us_gridded)\n",
    "        port_eu['d_in_euport'] = [1]*len(port_eu)\n",
    "        \n",
    "        print('Now completing us port intersection: ' + str(datetime.now()))\n",
    "        try:\n",
    "                points_with_intersection = gpd.sjoin(geo_df, \n",
    "                                            port_us_gridded[['geometry','PORT_NAME','d_in_usport']].rename(columns = {'PORT_NAME':'d_port_name_us'}), \n",
    "                                            how='left', op='intersects').reset_index().drop_duplicates(subset='index').set_index('index')\n",
    "                del points_with_intersection['index_right']\n",
    "        except:\n",
    "                points_with_intersection = geo_df\n",
    "                points_with_intersection['d_in_usport'] = 0\n",
    "\n",
    "        print('Now completing eu port intersection: ' + str(datetime.now()))\n",
    "        try:\n",
    "                points_with_intersection = gpd.sjoin(points_with_intersection, \n",
    "                                            port_eu[['geometry','PORT_NAME','d_in_euport']].rename(columns = {'PORT_NAME':'d_port_name_eu'}), \n",
    "                                            how='left', op='intersects').reset_index().drop_duplicates(subset='index').set_index('index')\n",
    "                del points_with_intersection['index_right']\n",
    "        except:\n",
    "                # points_with_intersection = geo_df\n",
    "                points_with_intersection['d_in_euport'] = 0\n",
    "\n",
    "        voyage=points_with_intersection\n",
    "        voyage.loc[voyage.d_in_usport.isnull(),'d_in_usport']=0\n",
    "        voyage.loc[voyage.d_in_euport.isnull(),'d_in_euport']=0\n",
    "        # del voyage['destination_geometry']\n",
    "\n",
    "        # dumping data before summarizing\n",
    "        voyage['uni_lid']=voyage['ship_id'].astype(str)+\"_\"+voyage['vid'].astype(str)+\"_\"+voyage['lid'].astype(str)\n",
    "        del voyage['geometry']\n",
    "        del voyage['geometry_x']\n",
    "        del voyage['d_port_name_us']\n",
    "        del voyage['d_port_name_eu']\n",
    "        del voyage['o_port_name_us']\n",
    "        del voyage['o_port_name_eu']\n",
    "        del voyage['vid']\n",
    "        del voyage['lid']\n",
    "        del voyage['traj_id']\n",
    "        del voyage['o_lon']\n",
    "        del voyage['o_lat']\n",
    "        del voyage['d_lon']\n",
    "        del voyage['d_lat']\n",
    "        # dump2pg(voyage,2022,'type1_processed_with_emission_usmrv_vid') # this step is to dump data into database, you don't need this process here\n",
    "\n",
    "        # summarize\n",
    "        lid_usmrv_outbound=list(voyage.loc[(voyage.o_in_usport==1)&(voyage.d_in_usport==0),'uni_lid'].unique())\n",
    "        lid_usmrv_inbound=list(voyage.loc[(voyage.o_in_usport==0)&(voyage.d_in_usport==1),'uni_lid'].unique())\n",
    "        lid_usmrv_cabotage=list(voyage.loc[(voyage.o_in_usport==1)&(voyage.d_in_usport==1),'uni_lid'].unique())\n",
    "        lid_usmrv_eumrv=list(voyage.loc[((voyage.o_in_usport==1)&(voyage.d_in_euport==1))|((voyage.d_in_usport==1)&(voyage.o_in_euport==1)),'uni_lid'].unique())\n",
    "        usmrv_lid_outbound.append(lid_usmrv_outbound)\n",
    "        usmrv_lid_inbound.append(lid_usmrv_inbound)\n",
    "        usmrv_lid_cabotage.append(lid_usmrv_cabotage)\n",
    "        usmrv_lid_eumrv.append(lid_usmrv_eumrv)\n",
    "        \n",
    "        # GWP co2:1 ch4-100:29.8 ch4-20:82.5 n2o:273 BC-100:900, BC-20:3200\n",
    "\n",
    "        df_outbound=voyage.loc[(voyage.d_in_usport==0)&(voyage.o_in_usport==1)]\n",
    "        df_inbound=voyage.loc[(voyage.d_in_usport==1)&(voyage.o_in_usport==0)]\n",
    "        df_cabotage=voyage.loc[(voyage.d_in_usport==1)&(voyage.o_in_usport==1)]\n",
    "        df_usmrv_eumrv=voyage.loc[voyage.uni_lid.isin(lid_usmrv_eumrv)]\n",
    "    \n",
    "        df_outbound_distance=df_outbound.distancetravelled_nm.sum()\n",
    "        df_outbound_fuel=df_outbound.fuelburned_g.sum()/1000000\n",
    "        df_outbound_co2=df_outbound.totalco2_g.sum()\n",
    "        df_outbound_co2e100_withoutbc=df_outbound.totalco2_g.sum() + df_outbound.totalch4_g.sum()*29.8 + df_outbound.totaln2o_g.sum()*273\n",
    "        df_outbound_co2e100_withbc=df_outbound.totalco2_g.sum() + df_outbound.totalch4_g.sum()*29.8 + df_outbound.totaln2o_g.sum()*273 + df_outbound.totalbc_g.sum()*900\n",
    "        df_outbound_co2e20_withoutbc=df_outbound.totalco2_g.sum() + df_outbound.totalch4_g.sum()*82.5 + df_outbound.totaln2o_g.sum()*273\n",
    "        df_outbound_co2e20_withbc=df_outbound.totalco2_g.sum() + df_outbound.totalch4_g.sum()*82.5 + df_outbound.totaln2o_g.sum()*273 +df_outbound.totalbc_g.sum()*3200\n",
    "        \n",
    "        df_inbound_distance=df_inbound.distancetravelled_nm.sum()\n",
    "        df_inbound_fuel=df_inbound.fuelburned_g.sum()/1000000\n",
    "        df_inbound_co2=df_inbound.totalco2_g.sum()\n",
    "        df_inbound_co2e100_withoutbc=df_inbound.totalco2_g.sum() + df_inbound.totalch4_g.sum()*29.8 + df_inbound.totaln2o_g.sum()*273\n",
    "        df_inbound_co2e100_withbc=df_inbound.totalco2_g.sum() + df_inbound.totalch4_g.sum()*29.8 + df_inbound.totaln2o_g.sum()*273 + df_inbound.totalbc_g.sum()*900\n",
    "        df_inbound_co2e20_withoutbc=df_inbound.totalco2_g.sum() + df_inbound.totalch4_g.sum()*82.5 + df_inbound.totaln2o_g.sum()*273\n",
    "        df_inbound_co2e20_withbc=df_inbound.totalco2_g.sum() + df_inbound.totalch4_g.sum()*82.5 + df_inbound.totaln2o_g.sum()*273 +df_inbound.totalbc_g.sum()*3200\n",
    "    \n",
    "        df_cabotage_distance=df_cabotage.distancetravelled_nm.sum()\n",
    "        df_cabotage_fuel=df_cabotage.fuelburned_g.sum()/1000000\n",
    "        df_cabotage_co2=df_cabotage.totalco2_g.sum()\n",
    "        df_cabotage_co2e100_withoutbc=df_cabotage.totalco2_g.sum() + df_cabotage.totalch4_g.sum()*29.8 + df_cabotage.totaln2o_g.sum()*273\n",
    "        df_cabotage_co2e100_withbc=df_cabotage.totalco2_g.sum() + df_cabotage.totalch4_g.sum()*29.8 + df_cabotage.totaln2o_g.sum()*273 + df_cabotage.totalbc_g.sum()*900\n",
    "        df_cabotage_co2e20_withoutbc=df_cabotage.totalco2_g.sum() + df_cabotage.totalch4_g.sum()*82.5 + df_cabotage.totaln2o_g.sum()*273\n",
    "        df_cabotage_co2e20_withbc=df_cabotage.totalco2_g.sum() + df_cabotage.totalch4_g.sum()*82.5 + df_cabotage.totaln2o_g.sum()*273 +df_cabotage.totalbc_g.sum()*3200\n",
    "    \n",
    "\n",
    "        df_usmrv_eumrv_distance=df_usmrv_eumrv.distancetravelled_nm.sum()\n",
    "        df_usmrv_eumrv_fuel=df_usmrv_eumrv.fuelburned_g.sum()/1000000\n",
    "        df_usmrv_eumrv_co2=df_usmrv_eumrv.totalco2_g.sum()\n",
    "        df_usmrv_eumrv_co2e100_withoutbc=df_usmrv_eumrv.totalco2_g.sum() + df_usmrv_eumrv.totalch4_g.sum()*29.8 + df_usmrv_eumrv.totaln2o_g.sum()*273\n",
    "        df_usmrv_eumrv_co2e100_withbc=df_usmrv_eumrv.totalco2_g.sum() + df_usmrv_eumrv.totalch4_g.sum()*29.8 + df_usmrv_eumrv.totaln2o_g.sum()*273 + df_usmrv_eumrv.totalbc_g.sum()*900\n",
    "        df_usmrv_eumrv_co2e20_withoutbc=df_usmrv_eumrv.totalco2_g.sum() + df_usmrv_eumrv.totalch4_g.sum()*82.5 + df_usmrv_eumrv.totaln2o_g.sum()*273\n",
    "        df_usmrv_eumrv_co2e20_withbc=df_usmrv_eumrv.totalco2_g.sum() + df_usmrv_eumrv.totalch4_g.sum()*82.5 + df_usmrv_eumrv.totaln2o_g.sum()*273 +df_usmrv_eumrv.totalbc_g.sum()*3200\n",
    "\n",
    "     \n",
    "        df_final.append([i,df_outbound_distance,df_inbound_distance,df_cabotage_distance,df_usmrv_eumrv_distance,\\\n",
    "                         df_outbound_fuel,df_inbound_fuel,df_cabotage_fuel,df_usmrv_eumrv_fuel,\\\n",
    "                         df_outbound_co2,df_inbound_co2,df_cabotage_co2,df_usmrv_eumrv_co2,\\\n",
    "                         df_outbound_co2e100_withbc,df_inbound_co2e100_withbc,df_cabotage_co2e100_withbc,df_usmrv_eumrv_co2e100_withbc,\\\n",
    "                         df_outbound_co2e100_withoutbc,df_inbound_co2e100_withoutbc,df_cabotage_co2e100_withoutbc,df_usmrv_eumrv_co2e100_withoutbc,\\\n",
    "                         df_outbound_co2e20_withbc,df_inbound_co2e20_withbc,df_cabotage_co2e20_withbc,df_usmrv_eumrv_co2e20_withbc,\\\n",
    "                         df_outbound_co2e20_withoutbc,df_inbound_co2e20_withoutbc,df_cabotage_co2e20_withoutbc,df_usmrv_eumrv_co2e20_withoutbc])\n",
    "    else:\n",
    "        print(\"no qualified points, move on\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid_cabotage_fixed= [x for x in usmrv_lid_cabotage if x]\n",
    "lid_cabotage_flat= [\n",
    "    x\n",
    "    for xs in lid_cabotage_fixed\n",
    "    for x in xs\n",
    "]\n",
    "lid_cabotage=pd.DataFrame(lid_cabotage_flat,columns=['uni_lid'])\n",
    "\n",
    "lid_outbound_fixed= [x for x in usmrv_lid_outbound if x]\n",
    "lid_outbound_flat= [\n",
    "    x\n",
    "    for xs in lid_outbound_fixed\n",
    "    for x in xs\n",
    "]\n",
    "lid_outbound=pd.DataFrame(lid_outbound_flat,columns=['uni_lid'])\n",
    "len(lid_outbound)\n",
    "\n",
    "usmrv_lid_inbound\n",
    "lid_inbound_fixed= [x for x in usmrv_lid_inbound if x]\n",
    "lid_inbound_flat= [\n",
    "    x\n",
    "    for xs in lid_inbound_fixed\n",
    "    for x in xs\n",
    "]\n",
    "lid_inbound=pd.DataFrame(lid_inbound_flat,columns=['uni_lid'])\n",
    "len(lid_inbound)\n",
    "\n",
    "usmrv_lid_eumrv\n",
    "lid_eumrv_fixed= [x for x in usmrv_lid_eumrv if x]\n",
    "lid_eumrv_flat= [\n",
    "    x\n",
    "    for xs in lid_eumrv_fixed\n",
    "    for x in xs\n",
    "]\n",
    "lid_eumrv=pd.DataFrame(lid_eumrv_flat,columns=['uni_lid'])\n",
    "len(lid_eumrv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lid_cabotage.to_csv('usmrv_cabotage_lid.csv',index=False)\n",
    "lid_eumrv.to_csv('usmrv_eumrv_lid.csv',index=False)\n",
    "lid_outbound.to_csv(\"usmrv_outbound_lid.csv\",index=False)\n",
    "lid_inbound.to_csv(\"usmrv_inbound_lid.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.DataFrame(df_final,columns=['ship_id','distance_outbound','distance_inbound','distance_cabotage','distance_usmrv_eumrv',\\\n",
    "                      'fuel_outbound_tonne','fuel_inbound_tonne','fuel_cabotage_tonne','fuel_usmrv_eumrv_tonne',\\\n",
    "                      'co2_outbound','co2_inbound','co2_cabotage','co2_usmrv_eumrv',\\\n",
    "                      'co2e100_outbound_wbc','co2e100_inbound_wbc','co2e100_cabotage_wbc','co2e100_usmrv_eumrv_wbc',\\\n",
    "                      'co2e100_outbound','co2e100_inbound','co2e100_cabotage','co2e100_usmrv_eumrv',\\\n",
    "                      'co2e20_outbound_wbc','co2e20_inbound_wbc','co2e20_cabotage_wbc','co2e20_usmrv_eumrv_wbc',\\\n",
    "                      'co2e20_outbound','co2e20_inbound','co2e20_cabotage','co2e20_usmrv_eumrv'])\n",
    "df_final.to_csv(\"usmrv_result_all.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_and_split (df):\n",
    "    # first make sure the following packages are installed\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import datetime\n",
    "        from datetime import datetime, timedelta\n",
    "        import movingpandas as mpd\n",
    "        import geopandas as gpd\n",
    "        from geopandas import GeoDataFrame\n",
    "        from matplotlib import pyplot as plt\n",
    "        from shapely.geometry import Point, LineString, Polygon\n",
    "        # from holoviews import opts, dim\n",
    "        # import hvplot.pandas\n",
    "    except:\n",
    "        get_ipython().system('pip install pandas')\n",
    "        import pandas as pd \n",
    "        get_ipython().system('pip install numpy')\n",
    "        import numpy as np \n",
    "        get_ipython().system('pip install datetime')\n",
    "        import datetime \n",
    "        from datetime import datetime, timedelta\n",
    "        get_ipython().system('pip install movingpandas') # please refer to movingpandas github (README) to properly install it\n",
    "        import movingpandas as mpd \n",
    "        get_ipython().system('pip install geopandas')\n",
    "        import geopandas as gpd\n",
    "        from geopandas import GeoDataFrame\n",
    "        get_ipython().system('pip install matplotlib')\n",
    "        from matplotlib import pyplot as plt\n",
    "        get_ipython().system('pip install shapely')\n",
    "        from shapely.geometry import Point, LineString, Polygon\n",
    "        # get_ipython().system('pip install holoviews')\n",
    "        # from holoviews import opts, dim\n",
    "        # get_ipython().system('pip install hvplot')\n",
    "        # import hvplot.pandas\n",
    "        \n",
    "    # define some important functions\n",
    "    def neighborhood(iterable):\n",
    "        iterator = iter(iterable)\n",
    "        prev_item = None\n",
    "        current_item = next(iterator)  # throws StopIteration if empty.\n",
    "        for next_item in iterator:\n",
    "            yield (prev_item, current_item, next_item)\n",
    "            prev_item = current_item\n",
    "            current_item = next_item\n",
    "        yield (prev_item, current_item, None)\n",
    "        \n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    def haversine(point1, point2, miles=False):\n",
    "        \"\"\" Calculate the great-circle distance between two points on the Earth surface.\n",
    "          :input: two 2-tuples, containing the latitude and longitude of each point\n",
    "          in decimal degrees.\n",
    "          Example: haversine((45.7597, 4.8422), (48.8567, 2.3508))\n",
    "          :output: Returns the distance bewteen the two points.\n",
    "          The default unit is kilometers. Miles can be returned\n",
    "          if the ``miles`` parameter is set to True.\n",
    "          \"\"\"\n",
    "      # unpack latitude/longitude\n",
    "        lat1, lng1 = point1\n",
    "        lat2, lng2 = point2\n",
    "\n",
    "      # convert all latitudes/longitudes from decimal degrees to radians\n",
    "        lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "\n",
    "      # calculate haversine\n",
    "        lat = lat2 - lat1\n",
    "        lng = lng2 - lng1\n",
    "        d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "        h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "        if miles:\n",
    "            return h * 0.621371  # in miles\n",
    "        else:\n",
    "            return h  # in kilometers\n",
    "   \n",
    "    # start by properly formating the timestamp variable\n",
    "    crs = 'epsg:4326'\n",
    "    df['timestamp']=pd.to_datetime(df['time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    df=df.set_index('timestamp')\n",
    "    df_berth=df.loc[df.phase.isin(['B','A'])]\n",
    "    df_cruise=df.loc[df.phase.isin(['M','C'])]\n",
    "    if len(df_cruise)>2:\n",
    "        geometry = [Point(xy) for xy in zip(df_cruise['lon'], df_cruise['lat'])]\n",
    "        geo_sp = gpd.GeoDataFrame(df_cruise, geometry=geometry)\n",
    "        traj=mpd.Trajectory(geo_sp,'ship_id')\n",
    "        trajs=mpd.ObservationGapSplitter(traj).split(gap=timedelta(hours=5)) # timedelta can be substituted to any number that makes sense to you. \n",
    "        # For good coverage ais data, timedelta can set to 2.\n",
    "\n",
    "        # Split the traj by stop gap\n",
    "        if len(trajs)>0:\n",
    "            df_new=pd.DataFrame()\n",
    "            id=0 # looping id for the following loop\n",
    "            lid=1 # for each new traj/ship, re-start the lid assignment\n",
    "            vid=1 # for each new traj/ship, re-start the vid assignment as well\n",
    "            voyage_new=[] # container for a group of legs belonging to one voyage\n",
    "            leg_new=[] # container for a group of legs belonging to one leg\n",
    "            trip_df=pd.DataFrame() # container for dataframe with assigned lid and vid\n",
    "            for trip_prev,trip,trip_next in neighborhood(trajs):           \n",
    "                #---------------------------------------------------------------------------------------\n",
    "                # First of all, fix short legs, assign leg id\n",
    "                #---------------------------------------------------------------------------------------\n",
    "                trip.df['tp']=pd.to_datetime(trip.df['time'],format='%Y-%m-%d %H:%M:%S')\n",
    "                trip.df=trip.df.sort_values(by='tp',ascending=True)\n",
    "                origin=trip.get_start_location()\n",
    "                dest=trip.get_end_location()\n",
    "                trip.df['O_lon']=origin.x\n",
    "                trip.df['O_lat']=origin.y\n",
    "                trip.df['D_lon']=dest.x\n",
    "                trip.df['D_lat']=dest.y\n",
    "                start_sog=trip.df.sog.iloc[0]\n",
    "                end_sog=trip.df.sog.iloc[-1]\n",
    "                trip.df['trip_length']=(trip.df.distancetravelled_nm.sum())*1.852 #unit is km\n",
    "                if id==0:\n",
    "                    # The first leg is itself a leg, a voyage\n",
    "                    trip.df['lid']='B'\n",
    "                    trip.df['vid']='B'\n",
    "                    voyage_new=[]\n",
    "                elif id==len(trajs)-1:\n",
    "                    # The last leg is itself a leg, a voyage\n",
    "                    trip.df['lid']='E'\n",
    "                    trip.df['vid']='E'\n",
    "                    voyage_new=[]\n",
    "                else:\n",
    "                    if (trip.df.distancetravelled_nm.sum())*1.852< 50:# this should be a normal port-to-port distance\n",
    "                        trip.df['lid']=lid\n",
    "                        trip.df['vid']=vid \n",
    "                    else:\n",
    "                        trip.df['lid']=lid\n",
    "                        trip.df['vid']=vid\n",
    "                        lid+=1\n",
    "                    voyage_new.append(trip)        \n",
    "                    try:\n",
    "                        df=pd.DataFrame()\n",
    "                        for v in voyage_new:\n",
    "                            d=v.df\n",
    "                            df=pd.concat([df,d],axis=0,join='outer')\n",
    "                        v_traj=mpd.TrajectoryCollection(df,'lid')\n",
    "                        print(\"voyage has {} legs.\".format(len(v_traj)))            \n",
    "                        voyage_angle=leg_angle(v_traj.trajectories[-1],v_traj.trajectories[0])\n",
    "                        voyage_dest=v_traj.trajectories[-1].get_end_location()\n",
    "                        voyage_origin=v_traj.trajectories[0].get_start_location()\n",
    "                    except:\n",
    "                        voyage_angle=0\n",
    "                        voyage_dest=dest\n",
    "                        voyage_origin=origin\n",
    "                    print(\"Voyage angle is {}.\".format(voyage_angle))\n",
    "\n",
    "                    distance_trip_od=haversine((dest.y,dest.x),(origin.y,origin.x)) #unit is km\n",
    "                    distance_voyage_od=haversine((voyage_dest.y,voyage_dest.x),(voyage_origin.y,voyage_origin.x))\n",
    "                    print (\"Voyage od is {}; voyage angle is {}, voyage now has {} legs.\".format(distance_voyage_od,voyage_angle,len(voyage_new)))\n",
    "\n",
    "                    if ((distance_trip_od<10) & ((trip.df.distancetravelled_nm.sum()*1.852)>10)) or (distance_voyage_od<10) or ((voyage_angle<10)&(len(voyage_new)>1)&(distance_voyage_od<15)):\n",
    "                        voyage_new=[]  \n",
    "                        vid+=1\n",
    "\n",
    "                trip_df=pd.concat([trip_df,trip.df],axis=0,join='outer')\n",
    "                id+=1        \n",
    "\n",
    "            trip_df_new=pd.concat([trip_df,df_berth],axis=0,join='outer')\n",
    "            trip_df_new['tp']=pd.to_datetime(trip_df_new['time'],format='%Y-%m-%d %H:%M:%S')\n",
    "            trip_df_new=trip_df_new.set_index('tp')\n",
    "            trip_df_new=trip_df_new.sort_values(by='tp',ascending=True)    \n",
    "            trip_df_new[['lid','vid','O_lon','O_lat','D_lon','D_lat']]=trip_df_new[['lid','vid','O_lon','O_lat','D_lon','D_lat']].fillna(method='ffill')\n",
    "            trip_df_new[['lid','vid','O_lon','O_lat','D_lon','D_lat']]=trip_df_new[['lid','vid','O_lon','O_lat','D_lon','D_lat']].fillna(method='bfill')\n",
    "            df_new=pd.concat([df_new,trip_df_new],axis=0,join='outer')\n",
    "            df_new=df_new.rename(columns={'O_lat':'o_lat','O_lon':'o_lon','D_lat':'d_lat','D_lon':'d_lon','trip_length':'trip_length_km'})\n",
    "            print(\"Extracted {} voyages from {} legs out of the original {} trips.\".format(vid,lid,len(trajs)))\n",
    "            return (df_new)\n",
    "        else:\n",
    "            print(\"This ship cannot get enough split of legs.\")\n",
    "            df[['lid','vid','o_lon','o_lat','d_lon','d_lat','trip_length_km']]=None\n",
    "            return (df)\n",
    "    else:\n",
    "        print(\"This ship has not enough points to form voyages.\")\n",
    "        df[['lid','vid','o_lon','o_lat','d_lon','d_lat','trip_length_km']]=None\n",
    "        return(df)\n",
    "\n",
    "\n",
    "def get_ship_from_pg(ship_id,year,start_time=None,end_time=None,lat_bound_l=None,lat_bound_h=None,lon_bound_l=None,lon_bound_h=None):\n",
    "    # Frist, make sure psycopg2 is installed\n",
    "    try:\n",
    "        import psycopg2\n",
    "        from psycopg2.extensions import register_adapter, AsIs\n",
    "        import time\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "    except:\n",
    "        get_ipython().system('pip install psycopg2-binary')\n",
    "        import psycopg2\n",
    "        from psycopg2.extensions import register_adapter, AsIs\n",
    "        get_ipython().system('pip install time')\n",
    "        import time\n",
    "        get_ipython().system('pip install numpy')\n",
    "        import numpy as np\n",
    "        get_ipython().system('pip install pandas')\n",
    "        import pandas as pd        \n",
    "    \n",
    "    # connect to psycopg2 to retreive data, code here uses 2019 ais data, which should be changed to relevant year    \n",
    "    db_name='ais_'+str(year)\n",
    "    try:\n",
    "        con = psycopg2.connect(host=\"ais-global-2017-2019.cdf3dcxncw9p.us-east-1.rds.amazonaws.com\", port=5432, database=db_name,user=\"postgres\",password=\"MarineTeamR0cks!\")\n",
    "        print(\"I have successfully connected to the database\")\n",
    "    except Exception as e:\n",
    "        print(\"I am unable to connect to the database:\" +str(e))\n",
    "        \n",
    "    # retrive data\n",
    "    try: \n",
    "        cur = con.cursor()   \n",
    "        def addapt_numpy_int64(numpy_int64):\n",
    "            return AsIs(numpy_int64)\n",
    "        register_adapter(np.int64, addapt_numpy_int64)\n",
    "        start_time = time.time()\n",
    "        print('Start time is: ' + str(start_time))\n",
    "        # the following command retrieves data by ship_id; however the function is built to be able to retrieve data\n",
    "        # within a certain time frame or geographical boundary. the code should be updated accordingly. However, \n",
    "        # retrieving time should be taken into account. it's not recommended to retrieve more than 2000 ships' annual\n",
    "        # ais data at a time.\n",
    "        command = cur.mogrify(\"SELECT * FROM type1_processed_with_emission_redo2 WHERE ship_id = (%s)\", [ship_id])\n",
    "        #print(command)\n",
    "        cur.execute(command)\n",
    "        print(\"Time to complete execution: \" + str(round((time.time()-start_time)/60,2)) + ' minutes')\n",
    "        start_time = time.time()\n",
    "        print('Start time is: ' + str(start_time))\n",
    "        rows = cur.fetchall() \n",
    "        colnames = [desc[0] for desc in cur.description]\n",
    "        print(\"Time to complete cur.fetchall() assignment to rows: \" + str(round((time.time()-start_time)/60,2)) + ' minutes')\n",
    "    except psycopg2.DatabaseError as e:\n",
    "        print(\"Error %s\" % e)\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "    Data = pd.DataFrame(rows, columns = colnames)\n",
    "\n",
    "    # process and return data\n",
    "    return (Data)\n",
    "\n",
    "def dump2pg(data,year,table_name):\n",
    "    import psycopg2\n",
    "    import io\n",
    "    from io import StringIO\n",
    "    from io import BytesIO\n",
    "    import time\n",
    "    import datetime\n",
    "    # First let's get rid of the remaining location identifiers\n",
    "    db_name='ais_'+str(year)    \n",
    "    \n",
    "    try:\n",
    "        connection = psycopg2.connect(host=\"ais-global-2017-2019.cdf3dcxncw9p.us-east-1.rds.amazonaws.com\", port=5432,\\\n",
    "                        database=db_name,user=\"postgres\",password=\"MarineTeamR0cks!\")\n",
    "        print(\"I have successfully connected to the database\")\n",
    "    except Exception as e:\n",
    "        print(\"I am unable to connect to the database:\" +str(e))\n",
    "    cur = connection.cursor()\n",
    "    start = time.time()            \n",
    "    try:\n",
    "        dump_obj=io.StringIO()\n",
    "        data.to_csv(dump_obj,sep=',',header=False, index=False)\n",
    "    except:\n",
    "        dump_obj=io.BytesIO()\n",
    "        data.to_csv(dump_obj,sep=',',header=False, index=False)         \n",
    "    \n",
    "    dump_obj.seek(0)\n",
    "    copy_command=\"COPY \"+ table_name +\" FROM STDOUT WITH DELIMITER ',' CSV\"\n",
    "    cur.copy_expert(copy_command,dump_obj)\n",
    "    connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
